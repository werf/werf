---
title: Сборка
permalink: usage/build.html
---

# Секция image

Образы описываются с помощью директивы _image_: `image: string`, с которой начинается описание образа в конфигурации.

```yaml
image: frontend
```

Если в файле конфигурации описывается только один образ, то он может быть безымянным:

```yaml
image: ~
```

Если в файле конфигурации описывается более одного образа, то **каждый образ** должен иметь собственное имя:

```yaml
image: frontend
...
---
image: backend
...
```

Образ может иметь несколько имен, указываемых в виде YAML-списка (это эквивалентно описанию нескольких одинаковых образов с разными именами):

```yaml
image: [main-front,main-back]
```

Имя образа требуется при использовании в helm-шаблонах, а также при запуске команд для определённого образа, описанного в `werf.yaml`.

## Сборщик Dockerfile

Сборка образа с использованием имеющегося Dockerfile — самый простой путь начать использовать werf в существующем проекте. Ниже приведен пример минимального файла `werf.yaml`, описывающего образ `example` проекта:

```yaml
project: my-project
configVersion: 1
---
image: example
dockerfile: Dockerfile
```

Также, вы можете описывать несколько образов из одного и того же Dockerfile:

```yaml
image: backend
dockerfile: Dockerfile
target: backend
---
image: frontend
dockerfile: Dockerfile
target: frontend
```

И конечно, вы можете описывать образы, основанные на разных Dockerfile:

```yaml
image: backend
dockerfile: dockerfiles/DockerfileBackend
---
image: frontend
dockerfile: dockerfiles/DockerfileFrontend
```

#### contextAddFiles

Контекст сборки Dockerfile-образа включает файлы, которые содержатся в директории, заданной директивой `context` (по умолчанию это директория проекта), из текущего коммита репозитория проекта.

Директива `contextAddFiles` позволяет дополнить контекст сборки произвольными файлами из директории проекта.

```yaml
image: app
context: app
contextAddFiles:
 - file1
 - dir1/
 - dir2/file2.out
```

В данной конфигурации контекст сборки будет состоять из следующих файлов:

- `app/**/*` из текущего коммита репозитория проекта;
- Файлы `app/file1`, `app/dir2/file2.out` и директория `dir1`, которые находятся в директории проекта.

Файлы `contextAddFiles` имеют больший приоритет, чем файлы из репозитория проекта, поэтому при пересечении пользователь будет работать с ними.

> По умолчанию, использование директивы `contextAddFiles` запрещено гитерминизмом (подробнее об этом в [статье]({{ "/advanced/giterminism.html#contextaddfiles" | true_relative_url }}))

## Stapel сборщик

Альтернативный способ сборки образов с использованием т.н. сборщика Stapel. Его особенности:

* Обеспечивает интеграцию с git и инкрементальную пересборку с учетом истории git-репозитория.
* Позволяет описывать инструкции сборки с помощью Ansible-заданий.
* Позволяет использовать между сборками общий кэш, с помощью монтирования.
* Позволяет уменьшить конечный размер образа, исключая из него исходный код и инструменты сборки.

# Стадии и хранилище

Мы разделили сборочный процесс образов, описанных в файле конфигурации [werf.yaml]({{ "reference/werf_yaml.html" | true_relative_url }}) на этапы, [с четкими функциями и назначением](#зависимости-стадии). Каждый такой этап соответствует промежуточному образу, подобно слоям в Docker. В werf такой этап называется [стадией](#конвеер-стадий), а **конечный образ** соответствует последней собранной стадии для определённого состояния git и конфигурации werf.yaml.

Стадии — это этапы сборочного процесса. ***Стадия*** определяется группой инструкций, указанных в конфигурации. Причем группировка этих инструкций не случайна, имеет определенную логику и учитывает условия и правила сборки. С каждой _стадией_ связан конкретный Docker-образ. Все стадии хранятся в [хранилище](#хранилище).

Вы можете рассматривать стадии как кэш сборки приложения, но в действительности это не совсем кэш, а часть сборочного контекста.

## Конвеер стадий

_Конвейер стадий_ — набор условий и правил выполнения стадий, подразумевающий также четко определенный порядок выполнения стадий. werf использует не один, а несколько _конвейеров стадий_ в своей работе, по-разному собирая образы в зависимости от их описанной конфигурации.

<div class="tabs">
  <a href="javascript:void(0)" class="tabs__btn active" onclick="openTab(event, 'tabs__btn', 'tabs__content', 'dockerfile-image-tab')">Dockerfile-образ</a>
  <a href="javascript:void(0)" class="tabs__btn" onclick="openTab(event, 'tabs__btn', 'tabs__content', 'stapel-image-tab')">Stapel-образ</a>
  <a href="javascript:void(0)" class="tabs__btn" onclick="openTab(event, 'tabs__btn', 'tabs__content', 'stapel-artifact-tab')">Stapel-артефакт</a>
</div>

<div id="dockerfile-image-tab" class="tabs__content active">
<a class="google-drawings" href="{{ "images/reference/stages_and_images1.png" | true_relative_url }}" data-featherlight="image">
<img src="{{ "images/reference/stages_and_images1_preview.png" | true_relative_url }}">
</a>
</div>

<div id="stapel-image-tab" class="tabs__content">
<a class="google-drawings" href="{{ "images/reference/stages_and_images2.png" | true_relative_url }}" data-featherlight="image">
<img src="{{ "images/reference/stages_and_images2_preview.png" | true_relative_url }}" >
</a>
</div>

<div id="stapel-artifact-tab" class="tabs__content">
<a class="google-drawings" href="{{ "images/reference/stages_and_images3.png" | true_relative_url }}" data-featherlight="image">
<img src="{{ "images/reference/stages_and_images3_preview.png" | true_relative_url }}">
</a>
</div>

**Пользователю нужно только написать правильную конфигурацию, остальная работа со стадиями выполняется werf.**

Для каждой _стадии_, werf подсчитывает уникальный сборочный идентификатор — [дайджест стадии](#дайджест-стадии).

В случае отсутствия у стадии [зависимостей стадии](#зависимости-стадии), она пропускается, и, соответственно, _конвейер стадий_ уменьшается на одну стадию. Таким образом конвейер стадий может уменьшаться, вплоть до единственной стадии _from_.

<a class="google-drawings" href="{{ "images/reference/stages_and_images4.png" | true_relative_url }}" data-featherlight="image">
<img src="{{ "images/reference/stages_and_images4_preview.png" | true_relative_url }}">
</a>

## Дайджест стадии

_Дайджест стадии_ используется для [тегирования](#именование-стадий) _стадии_ (дайджест является только частью тега) в _хранилище_.
werf не собирает стадию, если стадия с таким же _дайджестом_ уже находится в _хранилище_ (это поведение похоже на кэширование в Docker, только имеет более сложную логику).

***Дайджест стадии*** — это контрольная сумма от:
- контрольной суммы [зависимостей стадии](#зависимости-стадии).
- дайджеста предыдущей стадии;
- идентификатора git коммита связанного с предыдущей стадией (если эта стадия связана с git).

_Дайджест_ стадии идентифицирует содержимое стадии и зависит от истории правок в git, которые привели к этому коммиту.

## Зависимости стадии

_Зависимости стадии_ — это данные, которые напрямую связаны и влияют на [дайджест стадии](#дайджест-стадии). К зависимостям стадии относятся:

- файлы (и их содержимое) из git-репозиториев;
- инструкции сборки стадии из файла `werf.yaml`;
- произвольные строки указанные пользователем в `werf.yaml`
- и т.п.

Большинство _зависимостей стадии_ определяется в файле конфигурации `werf.yaml`, остальные — во время запуска.

Следующая таблица иллюстрирует зависимости в Dockerfile-образе, Stapel-образе и [Stapel-артефакте]({{ "advanced/building_images_with_stapel/artifacts.html" | true_relative_url }}).
Каждая строка таблицы описывает зависимости для определенной стадии. Левая колонка содержит краткое описание зависимостей, правая содержит соответствующую часть `werf.yaml` и ссылки на разделы с более подробной информацией.

<div class="tabs">
  <a href="javascript:void(0)" id="image-from-dockerfile-dependencies" class="tabs__btn dependencies-btn active">Dockerfile-образ</a>
  <a href="javascript:void(0)" id="image-dependencies" class="tabs__btn dependencies-btn">Stapel-образ</a>
  <a href="javascript:void(0)" id="artifact-dependencies" class="tabs__btn dependencies-btn">Stapel-артефакт</a>
</div>

<div id="dependencies">
{% for stage in site.data.stages.ru.entries %}
<div class="stage {{stage.type}}">
  <div class="stage-body">
    <div class="stage-base">
      <p>stage {{ stage.name | escape }}</p>

      {% if stage.dependencies %}
      <div class="dependencies">
        {% for dependency in stage.dependencies %}
        <div class="dependency">
          {{ dependency | escape }}
        </div>
        {% endfor %}
      </div>
      {% endif %}
    </div>

<div class="werf-config" markdown="1">

{% if stage.werf_config %}
```yaml
{{ stage.werf_config }}
```
{% endif %}

{% if stage.references %}
<div class="references">
    Подробнее:
    <ul>
    {% for reference in stage.references %}
        <li><a href="{{ reference.link | true_relative_url }}">{{ reference.name }}</a></li>
    {% endfor %}
    </ul>
</div>
{% endif %}

</div>

    </div>
</div>
{% endfor %}
</div>

<link rel="stylesheet" type="text/css" href="{{ assets["stages.css"].digest_path | true_relative_url }}" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script>
function application() {
  if ($("a[id=image-from-dockerfile-dependencies]").hasClass('active')) {
    $(".image").addClass('hidden');
    $(".artifact").addClass('hidden');
    $(".image-from-dockerfile").removeClass('hidden')
  }
  else if ($("a[id=image-dependencies]").hasClass('active')) {
    $(".image-from-dockerfile").addClass('hidden');
    $(".artifact").addClass('hidden');
    $(".image").removeClass('hidden')
  }
  else if ($("a[id=artifact-dependencies]").hasClass('active')) {
    $(".image-from-dockerfile").addClass('hidden');
    $(".image").addClass('hidden');
    $(".artifact").removeClass('hidden')
  }
  else {
    $(".image-from-dockerfile").addClass('hidden');
    $(".image").addClass('hidden');
    $(".artifact").addClass('hidden')
  }
}

$('.tabs').on('click', '.dependencies-btn', function() {
  $(this).toggleClass('active').siblings().removeClass('active');
  application()
});

application();
$.noConflict();
</script>

## Хранилище

_Хранилище_ хранит стадии и метаданные проекта. Эти данные могут храниться локально на хост-машине, либо в Docker Repo.

Большинство команд werf используют _стадии_. Такие команды требуют указания места размещения _хранилища_ с помощью ключа `--repo` или переменной окружения `WERF_REPO`.

Существует 2 типа хранилища:
1. _Локальное хранилище_. Использует локальный docker-server для хранения docker-образов.
2. _Удалённое хранилище_. Использует container registry для хранения docker-образов. Включается опцией `--repo=CONTAINER_REGISTRY_REPO`, например, `--repo=registry.mycompany.com/web`. **ЗАМЕЧАНИЕ** Каждый проект должен использовать в качестве хранилища уникальный адрес репозитория, который используется только этим проектом.

Стадии будут [именоваться по-разному](#именование-стадий) в зависимости от типа используемого хранилища.

При использовании container registry для хранения стадий, локальный docker-server на всех хостах, где запускают werf, используется как кеш. Этот кеш может быть очищен автоматически самим werf-ом, либо удалён с помощью других инструментов (например `docker rmi`) без каких-либо последствий.

### Именование стадий

Стадии в _локальном хранилище_ именуются согласно следующей схемы: `PROJECT_NAME:STAGE_DIGEST-TIMESTAMP_MILLISEC`. Например:

```
myproject                   9f3a82975136d66d04ebcb9ce90b14428077099417b6c170e2ef2fef-1589786063772   274bd7e41dd9        16 seconds ago      65.4MB
myproject                   7a29ff1ba40e2f601d1f9ead88214d4429835c43a0efd440e052e068-1589786061907   e455d998a06e        18 seconds ago      65.4MB
myproject                   878f70c2034f41558e2e13f9d4e7d3c6127cdbee515812a44fef61b6-1589786056879   771f2c139561        23 seconds ago      65.4MB
myproject                   5e4cb0dcd255ac2963ec0905df3c8c8a9be64bbdfa57467aabeaeb91-1589786050923   699770c600e6        29 seconds ago      65.4MB
myproject                   14df0fe44a98f492b7b085055f6bc82ffc7a4fb55cd97d30331f0a93-1589786048987   54d5e60e052e        31 seconds ago      64.2MB
```

Стадии в _удалённом хранилище_ именуются согласно следующей схемы: `CONTAINER_REGISTRY_REPO:STAGE_DIGEST-TIMESTAMP_MILLISEC`. Например:

```
localhost:5000/myproject-stages                 d4bf3e71015d1e757a8481536eeabda98f51f1891d68b539cc50753a-1589714365467   7c834f0ff026        20 hours ago        66.7MB
localhost:5000/myproject-stages                 e6073b8f03231e122fa3b7d3294ff69a5060c332c4395e7d0b3231e3-1589714362300   2fc39536332d        20 hours ago        66.7MB
localhost:5000/myproject-stages                 20dcf519ff499da126ada17dbc1e09f98dd1d9aecb85a7fd917ccc96-1589714359522   f9815cec0867        20 hours ago        65.4MB
localhost:5000/myproject-stages                 1dbdae9cc1c9d5d8d3721e32be5ed5542199def38ff6e28270581cdc-1589714352200   6a37070d1b46        20 hours ago        65.4MB
localhost:5000/myproject-stages                 f88cb5a1c353a8aed65d7ad797859b39d357b49a802a671d881bd3b6-1589714347985   5295f82d8796        20 hours ago        65.4MB
localhost:5000/myproject-stages                 796e905d0cc975e718b3f8b3ea0199ea4d52668ecc12c4dbf85a136d-1589714344546   a02ec3540da5        20 hours ago        64.2MB
```

- `PROJECT_NAME` — имя проекта;
- `CONTAINER_REGISTRY_REPO` — репозиторий, заданный опцией `--repo`;
- `STAGE_DIGEST` — дайджест стадии. Дайджест является идентификатором содержимого стадии и также зависит от истории правок в git репозитории, которые привели к такому содержимому.
- `TIMESTAMP_MILLISEC` — уникальный идентификатор, который генерируется в процессе [процедуры сохранения стадии]({{ "internals/build_process.html#сохранение-стадий-в-хранилище" | true_relative_url }}) после того как стадия была собрана.

# Организация хранилища стадий

В данной статье описано, как устроено хранилище собираемых образов в werf, какие бывают типы хранилища, какие функции выполняют эти хранилища, а также различные варианты организации хранилища в проекте.

## Хранилище стадий

werf собирает образы, состоящие из одной или нескольких стадий. Все стадии собираемых образов сохраняются в так называемое **хранилище стадий** по мере сборки.

Есть два вида хранилища стадий: **локальное** и **репозиторий** (container registry).

* Локальное хранилище стадий может быть использовано _только для локальной разработки_ и на данный момент только для сборки образов. В качестве локального хранилища выступает Docker server. Локальное хранилище стадий будет задействовано, например, если вызвать команду `werf build` без аргумента `--repo`.
* Во всех остальных случаях (т.е. кроме локальной разработки) в качестве хранилища стадий всегда выступает репозиторий в container registry. Если запустить сборку с хранилищем стадий в репозитории (например, `werf build` с параметром `--repo ghcr.io/example/myrepo`), то werf сначала проверит наличие требуемых стадий в локальном хранилище и скопирует оттуда подходящие стадии, чтобы не пересобирать эти стадии заново.

Для дальнейших подробностей введём также понятие **инсталляции проекта** — это Git-репозиторий проекта и связанные с ним хранилища стадий в репозитории. Во всех вызовах werf предполагается использование одного и того же Git-репозитория и одних и тех же хранилищ стадий *(может быть указано одно или несколько — подробности см. далее)*.

Также предполагается, что хранилище стадий инсталляции не может быть удалено или очищено сторонними средствами без негативных последствий для работы werf *(например, см. [первичный репозиторий](#первичный-репозиторий))*. Исключение составляют некоторые типы хранилищ, которые могут быть безопасно удалены в любой момент времени *(например, см. [кэширующий репозиторий](#кэширующий-репозиторий))*.

### Первичный репозиторий

* Задаётся параметром `--repo` (или переменной окружения `WERF_REPO`).
* У проекта всегда есть только один такой репозиторий.

Данное хранилище является основным и объединяет в себе несколько функций:

1. История проекта.
  - Хранилище метаданных, связанных с Git-репозиторием, для собираемых образов и стадий.
  - История проекта позволяет реализовать продвинутый алгоритм безопасной очистки старых стадий на основе истории Git-репозитория (команда `werf cleanup`).
2. Синхронизация сборки в распределённом окружении.
  - Данное хранилище позволяет запускать сборку в распределённом окружении и эффективно переиспользовать собираемые стадии в таком окружении. Это возможно за счёт механизмов распределённой синхронизации, встроенных в werf. Эти механизмы работают исключительно с первичным репозиторием.
  - Как только стадия попадает в первичный репозиторий, эта стадия может быть переиспользована другими сборочными процессами, которые могут быть запущены с произвольного хоста.
3. Сборочный кэш ранее собранных стадий.
  - Ранее собранные стадии по возможности переиспользуются вместо того, чтобы собираться заново.
  - Данная функция схожа с привычным локальным сборочным кэшом в Docker server, однако в случае werf этот кэш находится в container registry.
4. Хранилище финальных образов, используемых при запуске приложения в Kubernetes.
  - Kubernetes будет скачивать образы из этого репозитория для запуска контейнеров приложения.
  - В качестве финальных образов напрямую используются те же стадии, из которых состоит собранный образ.

**Очистка (ВАЖНО!).** Для корректной и полноценной работы werf первичный репозиторий должен быть надёжным (persistent) хранилищем, образы из которого удаляются лишь специальной командой очистки `werf cleanup`. Первичный репозиторий — это не просто сборочный кэш проекта, но и хранилище его истории, похожее на историю коммитов в Git-репозитории.

Помимо первичного репозитория в werf есть другие типы репозиториев, которые могут выполнять часть его функций. Они будут рассмотрены далее. Стоит помнить, что **без первичного репозитория werf не работает, а все другие репозитории — дополнительные**; они были созданы, чтобы снять часть функций первичного репозитория в вашей инсталляции проекта.

### Вторичный репозиторий

* Задаётся параметром `--secondary-repo` (или переменной окружения `WERF_SECONDARY_REPO_<NAME>`).
* Может быть указан один или несколько таких репозиториев.

Данное хранилище — это репозиторий только для чтения (read-only), выполняющий функцию сборочного кэша ранее собранных стадий *(см. функции [первичного репозитория](#первичный-репозиторий))*.

Как правило, в качестве secondary-repo может быть указан уже существующий первичный репозиторий от другой инсталляции проекта. Отсутствующие стадии по мере необходимости будут скопированы из вторичного репозитория в первичный вместо сборки с нуля. Вновь собранные же стадии будут помещены только в первичный репозиторий, потому что вторичный репозиторий всегда read-only, т.е. мы не имеем права в него записывать новые данные.

**Очистка** вторичного репозитория не требуется, т.к. этот репозиторий в то же время является первичным для другой инсталляции проекта и его данные управляются там.

### Кэширующий репозиторий

* Задаётся параметром `--cache-repo` (или переменной окружения `WERF_CACHE_REPO_<NAME>`).
* Может быть указан один или несколько таких репозиториев.

Выполняет функцию сборочного кэша ранее собранных стадий *(см. функции [первичного репозитория](#первичный-репозиторий))*. Кэширующий репозиторий отличается от вторичного тем, что он read-write, т.е. используется как для записи новых стадий в кэш, так и для использования собранных стадий из этого кэша.

Существующие стадии будут скопированы из кэширующего репозитория в первичный, поскольку первичный репозиторий обязан содержать все собранные стадии образов проекта. Вновь собранные стадии будут опубликованы и в первичный репозиторий, и в кэширующий репозиторий.

Стадии, которые используются в качестве базовых для сборки новых стадий:
* будут скачиваться из кэша,
* если их нет в кэширующем репозитории — будут скачиваться из первичного репозитория, а затем загружены в кэширующий для дальнейшего использования.

Другими словами, кэширующий репозиторий заполняется не только вновь собираемыми стадиями, но и базовыми стадиями, используемыми для сборки новых стадий.

Кэширующий репозиторий никогда не используется при запуске приложения в Kubernetes.

**Очистка** кэширующего репозитория осуществляется путём его полного удаления. После очистки такой репозиторий будет вновь наполнен актуальными часто используемыми данными.

### Финальный репозиторий

* Задаётся параметром `--final-repo` (или переменной окружения `WERF_FINAL_REPO`).
* Может быть указан только один такой репозиторий.

Данный репозиторий будет использоваться исключительно для публикации финальных образов, задействованных для деплоя приложения в Kubernetes.
- Вновь собираемые образы попадают сначала в первичный репозиторий, и затем копируются в финальный.
- Финальный репозиторий никогда не хранит промежуточные стадии образов, в нем есть лишь последняя стадия образа.
- Финальный репозиторий никогда не хранит промежуточные образы (артефакты), нужные для сборки конечных образов. Сюда попадают только конечные образы, используемые в Kubernetes.

**Очистка** данного репозитория выполняется командой `werf cleanup` в паре с первичным репозиторием (для этого необходимо указывать два параметра команде `werf cleanup`: `--repo` и `--final-repo`).

## Примеры организации хранилища стадий

Рассмотрим варианты использования и комбинации описанных репозиториев. Стоит отметить, что любой из описанных вариантов требует указания [первичного репозитория](#первичный-репозиторий).

### 1. Стандарт

* **Первичный** репозиторий доступен со всех сборочных хостов, а также из кластера Kubernetes. Другие репозитории не используются.
* При сборке образов сборочный кэш будет скачиваться и загружаться в первичный репозиторий. При выкате Kubernetes будет скачивать финальные образы приложения из данного репозитория.
* В большинстве случаев следует использовать именно эту простую схему. Также данная схема подходит, если нет уверенности в том, что требуется для проекта.

### 2. Дополнительный кэш в локальной сети

* **Первичный** репозиторий доступен со всех сборочных хостов, а также из кластера Kubernetes.
* **Кэширующий** репозиторий работает в локальной сети с быстрым доступом.
* При сборке образов сборочный кэш загружается и в первичный репозиторий, и в кэширующий репозиторий. Стадии образов, требуемые для сборки других стадий, будут скачиваться из кэширующего репозитория.

### 3. Полная оптимизация

* **Первичный** репозиторий располагается в локальной сети с быстрым доступом и доступен со всех сборочных хостов. Доступ к данному репозиторию из кластера Kubernetes не требуется. В отличие от кэширующего репозитория, первичный не может быть полностью очищен или удалён без последствий для работы werf, поэтому требуется обеспечить персистивность и надёжность данного хранилища.
* Опционально могут быть подняты дополнительные **кэширующие** репозитории в локальной сети с быстрым доступом.
* **Финальный** репозиторий располагается близко к кластеру Kubernetes для того, чтобы запускаемое приложение быстро скачивало образы приложения (скачивание финальных образов со стороны Kubernetes будет происходить чаще, чем их загрузка со сборочных хостов). Также к финальному репозиторию требуется доступ на запись со всех сборочных хостов. В финальный репозиторий будут загружены лишь конечные образы приложения (без промежуточных образов артефактов, которые нужны для сборки других образов).

# Процесс сборки

Сборочный процесс werf для образов, описанных в [werf.yaml]({{ "reference/werf_yaml.html" | true_relative_url }}), подразумевает [последовательную сборку стадий]({{ "internals/stages_and_storage.html" | true_relative_url }}#конвеер-стадий) для описанных образов.

Несмотря на то, что [_конвейеры стадий_]({{ "internals/stages_and_storage.html#конвеер-стадий" | true_relative_url }}) для Dockerfile-образа, Stapel-образа и Stapel-артефакта отличаются, каждая стадия подчиняется общим правилам [выборки из хранилища](#выборка-стадий), [сохранения](#сохранение-стадий-в-хранилище), а также [работы кеша и блокировок]({{ "advanced/synchronization.html" | true_relative_url }}) в параллельных запусках.

## Сборка стадии Dockerfile-образа

Для сборки Dockerfile-образа werf создает единственную [стадию]({{ "internals/stages_and_storage.html#конвеер-стадий" | true_relative_url }}) — `dockerfile`.

В настоящий момент, при сборке стадии werf использует стандартные команды встроенного в Docker клиента (это аналогично выполнению команды `docker build`), а также аргументы, которые пользователь описывает в `werf.yaml`. Кэш, создаваемый при сборке, используется, как и при обычной сборке, без помощи werf.

Интересной особенностью werf является то, что в качестве сборочного контекста используются файлы не из директории проекта, а из git-репозитория. Все файлы, которые содержатся в директории, заданной директивой `context` (по умолчанию это директория проекта), берутся из текущего коммита репозитория проекта (подробнее про гитерминизм можно почитать [в отдельной статье]({{ "advanced/giterminism.html#dockerfile-образ" | true_relative_url }})).

В итоге сборку стадии `dockerfile` можно представить следующим образом:

```shell
docker build --file=Dockerfile - < ~/.werf/service/tmp/context/4b9d6bc2-a549-42f9-86b8-4032c146f888
```

Подробнее о файле конфигурации сборки `werf.yaml` в [соответствующем разделе]({{ "reference/werf_yaml.html#сборщик-dockerfile" | true_relative_url }}).

## Сборка стадии Stapel-образа и Stapel-артефакта

При сборке стадии предполагается, что инструкции стадии будут запускаться в контейнере, основанном на предыдущей собранной стадии или на [базовом образе]({{ "advanced/building_images_with_stapel/base_image.html#from-fromlatest" | true_relative_url }}). Такой контейнер будет упоминаться далее как **сборочный контейнер**.

Перед запуском _сборочного контейнера_ werf подготавливает набор инструкций, который зависит от типа стадии и содержит как служебные команды werf, так и [пользовательские]({{ "advanced/building_images_with_stapel/assembly_instructions.html" | true_relative_url }}), указанные в конфигурации `werf.yaml`. Например, среди служебных команд может быть добавление файлов, наложение патчей, запуск ansible заданий и т.п.

Stapel-сборщик использует свой набор инструментов и библиотек и никак не зависит от базового образа. При запуске _сборочного контейнера_ werf монтирует всё необходимое из специального служебного образа `registry.werf.io/werf/stapel`. Подробнее об образе можно прочитать [в соответствующей статье]({{ "internals/development/stapel_image.html" | true_relative_url }}).

В _сборочный контейнер_ [пробрасывается сокет ssh-агента с хоста]({{ "internals/integration_with_ssh_agent.html" | true_relative_url }}), а также могут использоваться [пользовательские маунты]({{ "advanced/building_images_with_stapel/mount_directive.html" | true_relative_url }}).

Также стоит отметить, что при сборке werf игнорирует некоторые параметры манифеста базового образа, перетирая их определёнными значениями:
- `--user=0:0`;
- `--workdir=/`;
- `--entrypoint=/.werf/stapel/embedded/bin/bash`.

В итоге запуск _сборочного контейнера_ произвольной стадии можно представить следующим образом:
```shell
docker run \
  --volume=/tmp/ssh-ln8yCMlFLZob/agent.17554:/.werf/tmp/ssh-auth-sock \
  --volumes-from=stapel_0.6.1 \
  --env=SSH_AUTH_SOCK=/.werf/tmp/ssh-auth-sock \
  --user=0:0 \
  --workdir=/ \
  --entrypoint=/.werf/stapel/embedded/bin/bash \
  sha256:d6e46aa2470df1d32034c6707c8041158b652f38d2a9ae3d7ad7e7532d22ebe0 \
  -ec eval $(echo c2V0IC14 | /.werf/stapel/embedded/bin/base64 --decode)
```

Подробнее о файле конфигурации сборки `werf.yaml` в [соответствующем разделе]({{ "reference/werf_yaml.html#stapel-сборщик" | true_relative_url }}).

### Как Stapel-сборщик работает с CMD и ENTRYPOINT

Для сборки стадии werf запускает контейнер со служебными значениями `CMD` и `ENTRYPOINT` а затем, заменяет их значениями [базового образа]({{ "advanced/building_images_with_stapel/base_image.html" | true_relative_url }}). Если в базовом образе эти значения не установлены, werf сбрасывает их следующим образом:
- `[]` для `CMD`;
- `[""]` для `ENTRYPOINT`.

Также werf сбрасывает (использует специальные пустые значения) значение `ENTRYPOINT` базового образа, если указано значение `CMD` в конфигурации (`docker.CMD`).

В противном случае поведение werf аналогично [поведению Docker](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).

### Как использовать Stapel-сборщик в закрытом контуре

При запуске _сборочного контейнера_ werf монтирует инструментарий из специального образа, имя и тег которого зашиты в коде.

Для сборки со Stapel в закрытом контуре (без доступа к служебному образу) необходимо загрузить образ в доступный container registry и переопределить имя с помощью переменных окружения `WERF_STAPEL_IMAGE_NAME` и `WERF_STAPEL_IMAGE_VERSION` (например, `WERF_STAPEL_IMAGE_NAME=localhost:5000/stapel` и `WERF_STAPEL_IMAGE_VERSION=0.6.2`).

## Выборка стадий

Алгоритм выборки стадии в werf можно представить следующим образом:

1. Рассчитывается [дайджест стадии]({{ "internals/stages_and_storage.html#дайджест-стадии" | true_relative_url }}).
2. Выбираются все стадии, подходящие под дайджест, т.к. с одним дайджестом может быть связанно несколько стадий в [хранилище]({{ "internals/stages_and_storage.html#хранилище" | true_relative_url }}).
3. Выбирается старейший по времени `TIMESTAMP_MILLISEC` (подробнее про именование стадий [здесь]({{ "internals/stages_and_storage.html#именование-стадий" | true_relative_url }})).

### Дополнение для Stapel-образов и Stapel-артефактов

К основному алгоритму добавляется проверка родства git-коммитов. После шага **2** выполняется дополнительный отсев с использованием истории git: если текущая стадия связана с git (стадия git-архив, пользовательская стадия с git-патчами или стадия git latest patch), тогда выбираются только те стадии, которые связаны с коммитами, являющимися предками текущего коммита. Таким образом, коммиты соседних веток будут отброшены.

Возможна ситуация когда существует несколько собранных образов с одинаковым дайджестом. Более того, стадии для разных git-веток могут иметь одинаковую дайджест. Однако werf гарантированно предотвращает переиспользование кеша между несвязанными ветками. Кеш в разных ветках может быть переиспользован только если этот кеш относится к коммиту, который является базовым, как для одной ветки, так и для другой.

## Сохранение стадий в хранилище

Множество процессов werf (на одном хосте или на нескольких хостах) могут инициировать сборку одной и той же стадии в один момент времени, потому что этой стадии еще нет в хранилище.

werf использует алгоритм оптимистичных блокировок в процессе сохранения свежесобранного образа в хранилище. Когда сборка нового образа закончена, werf блокирует хранилище на любые операции с целевым дайджестом:
- Если за время сборки подходящего образа не появилось в хранилище, то werf сохраняет новый образ, сгенерировав гарантированно уникальный идентификатор `TIMESTAMP_MILLISEC`.
- Если за время сборки подходящий образ появился в хранилище, то werf отбрасывает свежесобранный образ, а вместо него использует появившийся в хранилище образ.

Другими словами: первый процесс, который закончит сборку новой стадии (самый быстрый процесс) получит шанс сохранить собранный образ в хранилище. Медленный процесс сборки не будет блокировать более быстрые процессы в параллельной и распределенной среде.

В процессе выборки и сохранения новых стадий в хранилище werf использует [менеджер блокировок]({{ "advanced/synchronization.html" | true_relative_url }}) для координации работы нескольких процессов werf.

## Параллельная сборка

Параллельная сборка в werf регулируется двумя параметрами `-p, --parallel` и `--parallel-tasks-limit`. По умолчанию параллельная сборка включена и собирается не более 5 образов одновременно.

После построение дерева зависимостей образов, werf разбивает сборку на этапы. Каждый этап содержит набор независимых образов, которые могут собираться параллельно.

```shell
┌ Concurrent builds plan (no more than 5 images at the same time)
│ Set #0:
│ - ⛵ image common-base
│ - 🛸 artifact jq
│ - 🛸 artifact libjq
│ - 🛸 artifact kcov
│
│ Set #1:
│ - ⛵ image base-for-go
│
│ Set #2:
│ - 🛸 artifact terraform-provider-vsphere
│ - 🛸 artifact terraform-provider-gcp
│ - 🛸 artifact candictl
│ - ⛵ image candictl-tests
│ - 🛸 artifact helm
│ - 🛸 artifact controller
│
│ Set #3:
│ - ⛵ image base
│
│ Set #4:
│ - ⛵ image tests
│ - ⛵ image app
└ Concurrent builds plan (no more than 5 images at the same time)
```

## Buildah

werf поддерживает использование Buildah в экспериментальном режиме для сборки образов.

При этом процесс сборки не изменился, перемены коснулись только бэкенда контейнеров и хранилища (сервер Docker был заменен на Buildah).

Более подробную информацию о Buildah можно получить в разделе [Режим сборки с использованием Buildah.]({{ "/advanced/buildah_mode.html" | true_relative_url }})

# Режим сборки с использованием Buildah

> ПРИМЕЧАНИЕ: werf поддерживает сборку образов с _использованием Docker-сервера_ или _с использованием Buildah_. Поддерживается сборка как Dockerfile-образов, так и stapel-образов через Buildah.

Для сборки без Docker-сервера werf использует встроенный Buildah в rootless-режиме.

## Системные требования

Требования к хост-системе для запуска werf в Buildah-режиме без Docker/Kubernetes можно найти в [инструкциях по установке](/installation.html). А для запуска werf в Kubernetes или в Docker-контейнерах требования следующие:
* Если ваше ядро Linux версии 5.13+ (в некоторых дистрибутивах 5.11+) **рекомендуется** режим работы через модуль ядра `overlay`:
  * Убедитесь, что модуль ядра `overlay` загружен с `lsmod | grep overlay`.
  * Убедитесь, что настройка ядра `CONFIG_USER_NS=y` включена в вашем ядре с помощью `grep CONFIG_USER_NS /boot/config-VERSION`.
  * При использовании ядра в debian-системах команда `sysctl kernel.unprivileged_userns_clone` должна вернуть `1`. В ином случае выполните:
    ```shell
    echo 'kernel.unprivileged_userns_clone = 1' | sudo tee -a /etc/sysctl.conf
    sudo sysctl -p
    ```
  * Команда `sysctl user.max_user_namespaces` должна вернуть по меньшей мере `15000`. В ином случае выполните:
    ```shell
    echo 'user.max_user_namespaces = 15000' | sudo tee -a /etc/sysctl.conf
    sudo sysctl -p
    ```
* Если ядро более старое или у вас не получается активировать модуль ядра `overlay`, то установите `fuse-overlayfs`, который обычно доступен в репозиториях вашего дистрибутива. В крайнем случае может быть использован драйвер хранилища `vfs`.

## Включение Buildah

Buildah включается установкой переменной окружения `WERF_BUILDAH_MODE` в один из вариантов: `auto`, `native-chroot`, `native-rootless`. Большинству пользователей для включения режима Buildah достаточно установить `WERF_BUILDAH_MODE=auto`.

* `auto` — автоматический выбор режима в зависимости от платформы и окружения;
* `native-chroot` работает только в Linux и использует `chroot`-изоляцию для сборочных контейнеров;
* `native-rootless` работает только в Linux и использует `rootless`-изоляцию для сборочных контейнеров. На этом уровне изоляции werf использует среду выполнения сборочных операций в контейнерах (runc, crun, kata или runsc).

> ПРИМЕЧАНИЕ: На данный момент Buildah доступен только для пользователей Linux и Windows с включённой подсистемой WSL2. Для пользователей MacOS на данный момент предлагается использование виртуальной машины для запуска werf в режиме Buildah.

## Драйвер хранилища

werf может использовать драйвер хранилища `overlay` или `vfs`:

* `overlay` позволяет использовать файловую систему OverlayFS. Можно использовать либо встроенную в ядро Linux поддержку OverlayFS (если она доступна), либо реализацию fuse-overlayfs. Это рекомендуемый выбор по умолчанию.
* `vfs` обеспечивает доступ к виртуальной файловой системе вместо OverlayFS. Эта файловая система уступает по производительности и требует привилегированного контейнера, поэтому ее не рекомендуется использовать. Однако в некоторых случаях она может пригодиться.

Как правило, достаточно использовать драйвер по умолчанию (`overlay`). Драйвер хранилища можно задать с помощью переменной окружения `WERF_BUILDAH_STORAGE_DRIVER`.

## Ulimits

По умолчанию Buildah режим в werf наследует системные ulimits при запуске сборочных контейнеров. Пользователь может переопределить эти параметры с помощью переменной окружения `WERF_BUILDAH_ULIMIT`.

Формат `WERF_BUILDAH_ULIMIT=type:softlimit[:hardlimit][,type:softlimit[:hardlimit],...]` — конфигурация лимитов, указанные через запятую:
* "core": maximum core dump size (ulimit -c)
* "cpu": maximum CPU time (ulimit -t)
* "data": maximum size of a process's data segment (ulimit -d)
* "fsize": maximum size of new files (ulimit -f)
* "locks": maximum number of file locks (ulimit -x)
* "memlock": maximum amount of locked memory (ulimit -l)
* "msgqueue": maximum amount of data in message queues (ulimit -q)
* "nice": niceness adjustment (nice -n, ulimit -e)
* "nofile": maximum number of open files (ulimit -n)
* "nproc": maximum number of processes (ulimit -u)
* "rss": maximum size of a process's (ulimit -m)
* "rtprio": maximum real-time scheduling priority (ulimit -r)
* "rttime": maximum amount of real-time execution between blocking syscalls
* "sigpending": maximum number of pending signals (ulimit -i)
* "stack": maximum stack size (ulimit -s)
