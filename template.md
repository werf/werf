---
author_team: "bravo"
author_name: "Кот Васька"
ci: "gitlab"
language: "python"
framework: "flask"
is_compiled: 0
package_managers_possible:
 - pip
package_managers_chosen: "pip"
unit_tests_possible:
 - flask-sqlalchemy
 - pytest
 - unittest
 - nose
 - nose2
unit_tests_chosen: "flask-sqlalchemy"
assets_generator_possible:
 - webpack
 - gulp
assets_generator_chosen: "webpack"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут {{Python программисту собрать приложение на Flask}} и запустить его в kubernetes кластере.

Предполагается что читатель имеет базовые знания в разработке на {{Python и Flask}} а также немного знаком с Gitlab CI и примитивами kubernetes, либо готов во всём этом разобраться самостоятельно. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - также приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в Kubernetes - если они соответствуют [12 факторам heroku](https://12factor.net/). Благодаря этому - у нас в kubernetes работают stateless приложения, которые не зависят от среды. Это важно, так как кластер может самостоятельно переносить приложения с одного узла на другой, заниматься масштабированием и т.п. — и мы не указываем, где конкретно запускать приложение, а лишь формируем правила, на основании которого кластер принимает свои собственные решения.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Для того, чтобы пройти по этому гайду, необходимо, чтобы

*   У вас был работающий и настроенный Kubernetes кластер
*   Код приложения находился в Gitlab
*   Был настроен Gitlab CI, подняты и подключены к нему раннеры

Для пользователя под которым будет производиться запуск runner-а - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf и автоматически обновлять его. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для автоматического выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:

```
. $(multiwerf use 1.1 stable --as-file)
```

Перед деплоем нашего приложения необходимо убедиться что у нас подготовлены инфраструктурные компоненты:

*   К gitlab подключен shell runner с тегом werf. [Инструкция](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке gitlab runner.
*   Ранеры включены и активны для репозитория с нашим приложением
*   Для пользователя под которым запускается сборка и деплой установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry
*   Раннер запущен на отдельной виртуалке, имеет доступ к API kubernetes и запускается по тегу werf

## Настройка Gitlab Runner

{{TODO: откорректировать то, что ниже}}

Теперь обязательно на сервере с gitlab-runner, который занимается сборкой вашего приложения, установить [kubectl](https://kubernetes.io/ru/docs/tasks/tools/install-kubectl/). И положить в домашнюю директорию пользователя gitlab-runner конфиг kubernetes в папку `.kube` (нужно её создать, если её нет)

Конфиг можно взять на мастере кластера kubernetes в `/etc/kubernetes/admin.conf`

Скопируйте его и положите в папку `.kube` переименовав файл в `config`.

![alt_text](images/-5.png "image_tooltip")

Обычно в конфиге указан прямой адрес мастера kubernetes. Соответственно нужно чтобы мастер нашего кластера был доступен по сети для gitlab-runner.

Всё это мы сделали для того чтобы werf мог общаться с API kubernetes и деплоить в него наши приложения, это и есть второй ответ на вопрос “Как werf понимает куда ему нужно деплоить?”. По умолчанию деплой будет происходить в namespace состоящий из имени проекта задаваемого в `werf.yaml` и окружения задаваемого в `.gitlab-ci.yml` куда мы деплоим наше приложение.
# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf. Его единственной задачей будет вывод сообщения “hello world” по http.

{{В этом образе будет работать один основной процесс gunicorn, который запустит приложение через wsgi.}} 

Управлять маршрутизацией запросов к приложению будет управлять Ingress в kubernetes кластере.

Мы реализуем два стенда: production и staging. В рамках hello world приложения мы предполагаем, что разработка ведётся локально, на вашем компьютере.

_В ближайшее время werf реализует удобные инструменты для локальной разработки, следите за обновлениями._


## Локальная сборка

{{Где мы берём исходники приложения}}

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки.

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.

Итак, начнём с самой главной секции нашего werf.yaml файла, которая должна присутствовать в нём **всегда**. Называется она [meta config section](https://werf.io/documentation/configuration/introduction.html#meta-config-section) и содержит всего два параметра.

werf.yaml:
```yaml
project: {{chat}}
configVersion: 1
```

**_project_** - поле, задающее имя для проекта, которым мы определяем связь всех docker images собираемых в данном проекте. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

**_configVersion_** - в данном случае определяет версию синтаксиса используемую в `werf.yaml`.

После мы сразу переходим к следующей секции конфигурации, которая и будет для нас основной секцией для сборки - [image config section](https://werf.io/documentation/configuration/introduction.html#image-config-section). И чтобы werf понял что мы к ней перешли разделяем секции с помощью тройной черты.


```yaml
project: chat
configVersion: 1
---
{{image: jopa}}
{{from: jopa:14-01-03}}
```

{{Блок image и связанное с ним}}

Теперь встает вопрос о том как нам добавить исходный код приложения внутрь нашего docker image. И для этого мы можем использовать Git! И нам даже не придётся устанавливать его внутрь docker image.

**_git_**, на наш взгляд это самый правильный способ добавления ваших исходников внутрь docker image, хотя существуют и другие. Его преимущество в том что он именно клонирует, и в дальнейшем накатывает коммитами изменения в тот исходный код что мы добавили внутрь нашего docker image, а не просто копирует файлы. Вскоре мы узнаем зачем это нужно.

```yaml
project: chat
configVersion: 1
---
{{image: jopa}}
{{from: jopa:14-01-03}}
git:
- add: /
  to: /app
```

Werf подразумевает что ваша сборка будет происходить внутри директории склонированного git репозитория. Потому мы списком можем указывать директории и файлы относительно корня репозитория которые нам нужно добавить внутрь image.

`add: /` - та директория которую мы хотим добавить внутрь docker image, мы указываем, что это весь наш репозиторий

`to: /app` - то куда мы клонируем наш репозиторий внутри docker image. Важно заметить что директорию назначения werf создаст сам.

 Есть возможность даже добавлять внешние репозитории внутрь проекта не прибегая к предварительному клонированию, как это сделать можно узнать [тут](https://werf.io/documentation/configuration/stapel_image/git_directive.html), но мы не рекомендуем такой подход.

{{Блок ansible и вот это всё, что в него входит:}}
{{*   Кратко объяснить, про стадии, но объяснить, что подробнее мы будем смотреть позже. Пока что обозначаем одну-две стадии и говорим, что вот они такие.}}
{{*   Про то, как мы дёргаем ансибл и что-то доустанавливаем.}}
{{*   ЕСЛИ есть компиляция - вот тут будем её показывать}}

Полный список поддерживаемых модулей ansible в werf можно найти [тут](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#supported-modules).

Не забыв [установить werf](https://werf.io/documentation/guides/installation.html) локально, запускаем сборку с помощью [werf build](https://werf.io/documentation/cli/main/build.html)!

```
$  werf build --stages-storage :local
```

![alt_text](images/-0.gif "image_tooltip")

Вот и всё, наша сборка успешно завершилась. К слову если сборка падает и вы хотите изнутри контейнера её подебажить вручную, то вы можете добавить в команду сборки флаги:

```
--introspect-before-error
```

или

```
--introspect-error
```

Которые при падении сборки на одном из шагов автоматически откроют вам shell в контейнер, перед исполнением проблемной инструкции или после.

В конце werf отдал информацию о готовом image:

![alt_text](images/-1.png "image_tooltip")

Теперь его можно запустить локально используя image_id просто с помощью docker.
Либо вместо этого использовать [werf run](https://werf.io/documentation/cli/main/run.html):


```
werf run --stages-storage :local --docker-options="-d -p 8080:8080 --restart=always" -- {{node /app/src/js/index.js}}
```

Первая часть команды очень похожа на build, а во второй мы задаем [параметры](https://docs.docker.com/engine/reference/run/) docker и через двойную черту команду с которой хотим запустить наш image.

Небольшое пояснение про `--stages-storage :local `который мы использовали и при сборке и при запуске приложения. Данный параметр указывает на то где werf хранить стадии сборки. На момент написания статьи это возможно только локально, но в ближайшее время появится возможность сохранять их в registry.

Теперь наше приложение доступно локально на порту 8080:

![alt_text](images/-2.png "image_tooltip")

На этом часть с локальным использованием werf мы завершаем и переходим к той части для которой werf создавался, использовании его в CI.

## Построение CI-процесса

После того как мы закончили со сборкой, которую можно производить локально, мы приступаем к базовой настройке CI/CD на базе Gitlab.

Начнем с того что добавим нашу сборку в CI с помощью .gitlab-ci.yml, который находится внутри корня проекта. Нюансы настройки CI в Gitlab можно найти [тут](https://docs.gitlab.com/ee/ci/).

Мы предлагаем простой флоу, который мы называем [fast and furious](https://docs.google.com/document/d/1a8VgQXQ6v7Ht6EJYwV2l4ozyMhy9TaytaQuA9Pt2AbI/edit#). Такой флоу позволит вам осуществлять быструю доставку ваших изменений в production согласно методологии GitOps и будут содержать два окружения, production и stage.

На стадии сборки мы будем собирать образ с помощью werf и загружать образ в registry, а затем на стадии деплоя собрать инструкции для kubernetes, чтобы он скачивал нужные образы и запускал их.

### Сборка в Gitlab CI

Для того, чтобы настроить CI-процесс создадим .gitlab-ci.yaml в корне репозитория

{{Версию верфи выносим в переменную}}

{{Сборку стадий прописываем в WERF_STAGES_STORAGE}}

{{Выносим подключение верфи в before_script (вот эти type multiwerf и type werf)}}

{{В самой билд стадии делаем werf build-and-publish}}

{{Если есть компиляция — описываем как её делать}}

{{Про content-based tagging}}

{{Отдельно проговариваем историю с проброской конфигов в стадию сборки.}} 

Теперь мы можем запушить наши изменения и увидеть что наша стадия успешно выполнилась.

![alt_text](images/-3.png "image_tooltip")


Лог в Gitlab будет выглядеть так же как и при локальной сборке, за исключением того что в конце мы увидим как werf пушит наш docker image в registry.

```
207 │ ┌ Publishing image {{node}} by stages-signature tag c905b748cb9647a03476893941837bf79910ab09e ...
208 │ ├ Info
209 │ │   images-repo: registry.gitlab-example.com/{{chat/node}}
210 │ │        image: registry.gitlab-example.com/{{chat/node}}:c905b748cb9647a03476893941 ↵
211 │ │   837bf79910ab09ef5878037592a45d
212 │ └ Publishing image {{node}} by stages-signature tag c905b748cb9647a0347689394 ... (14.90 seconds)
213 └ ⛵ image {{node}} (73.44 seconds)
214 Running time 73.47 seconds
218 Job succeeded
```

### Деплой

werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как файлы c секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

Werf (по аналогии с helm) берет yaml шаблоны, генерирует из них  огромную простыню с финальными ямлами, куда подставлены все значения. В этой простыне ямла — аннотации для кубернетеса. Эта простыня закидывается в кубернетес кластер, который парсит инструкции в ямле и вносит изменения в кластер. Верфь смотрит за тем, как кубернетес вносит изменения и дожидается, чтобы реально всё было применено.

Внутри Werf доступны команды Helm-а, например, проверить какие файлы получаются в результате работы werf с шаблонами можно выполнив команду рендер:

```
$ werf helm render
```

Аналогично, доступны команды [helm list](https://werf.io/documentation/cli/management/helm/list.html) и другие.

#### Общее про хельм-конфиги

На сегодняшний день [Helm](https://helm.sh/) один из самых удобных способов которым вы можете описать свой deploy в Kubernetes. Кроме возможности установки готовых чартов с приложениями прямиком из репозитория, где вы можете введя одну команду, развернуть себе готовый Redis, Postgres, Rabbitmq прямиком в Kubernetes, вы также можете использовать Helm для разработки собственных чартов с удобным синтаксисом для шаблонизации выката ваших приложений.

Потому для werf это был очевидный выбор использовать такую технологию.

Мы не будем вдаваться в подробности разработки yaml манифестов с помощью Helm для Kubernetes. Осветим лишь отдельные её части, которые касаются данного приложения и werf в целом. Если у вас есть вопросы о том как именно описываются объекты Kubernetes, советуем посетить страницы документации по Kubernetes с его [концептами](https://kubernetes.io/ru/docs/concepts/) и страницы документации по разработке [шаблонов](https://helm.sh/docs/chart_template_guide/) в Helm.

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm (здесь мы будем описывать деплой)
├── templates (объекты kubernetes в виде шаблонов)
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файл с переменными для параметризации шаблонов)
```

Подробнее читайте в [нашей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

![alt_text](images/-4.png "image_tooltip")

#### Описание приложения в хельме

Для работы нашего приложения в среде Kubernetes понадобится описать сущности Deployment, Service и завернуть трафик на приложение, донастроив роутинг в кластере.

Не забываем создать валидный ключ для доступа из kubernetes к registry gitlab.

```yaml
      imagePullSecrets:
      - name: registrysecret
```

##### Запуск контейнера

{{Про запускаемый command}}
{{Про werf_container_image и то, для чего он нужен}}

##### Переменные окружения

Для корректной работы нашего приложения ему нужно узнать переменные окружения.
Для {{Frameworkname}} это, например, {{рассказ про то, какие и зачем}}.

И эти переменные можно параметризовать с помощью файла `values.yaml`.

{{Так например, мы пробросим значение переменной такой-то вот так из оттуда

```yaml
      env:
      - name: RAILS_MASTER_KEY
        value: {{ .Values.rails.master_key}}
      - name: RAILS_ENV
        value: production
```
}}

Переменные окружения иногда используются для того, чтобы не перевыкатывать контейнеры, которые не менялись.

Werf закрывает ряд вопросов, связанных с перевыкатом контейнеров с помощью конструкции  [werf_container_env](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_env). Она возвращает блок с переменной окружения DOCKER_IMAGE_ID контейнера пода. Значение переменной будет установлено только если .Values.global.werf.is_branch=true, т.к. в этом случае Docker-образ для соответствующего имени и тега может быть обновлен, а имя и тег останутся неизменными. Значение переменной DOCKER_IMAGE_ID содержит новый ID Docker-образа, что вынуждает Kubernetes обновить объект.

```yaml
{{ tuple "rails" . | include "werf_container_env" | indent 8 }}
```

Аналогично можно пробросить секретные переменные (пароли и т.п.) и у Верфи есть специальный механизм для этого. Но к этому вопросу мы вернёмся позже.

##### Логгирование

{{ОБЯЗАТЕЛЬНО написать про то, что логи в stdout и как это сделать в этом фреймворке (не исключает использования Sentry-подобных штук, о чём будет позже)}}


##### Роутинг и заворачивание трафика на приложение

Нам надо будет пробить порт у пода, сервиса и настроить Ingress, который выступает у нас в качестве балансера.

Если вы мало работали с Kubernetes — эта часть может вызвать у вас много проблем. Большинство тех, кто начинает работать с Kubernetes по невнимательности допускают ошибки при конфигурировании labels и затем занимаются долгой и мучительной отладкой.

{{TODO: тут бы дать какую-то подсказку, как человеку пройти через это и не поседеть, если у него рядом нет ментора, который ткнёт ему в опечатку}}

###### Прокрутить порт

{{кратко про то, как мы прокручиваем порт, прописывая нужное в поде и в сервисе}}

###### Роутинг на Ingress

{{Прописываем роутинг}}
{{Проброска ci_url и какова логика}}

#### Секретные переменные

Для хранения в репозитории паролей, файлов сертификатов и т.п., рекомендуется использовать подсистему работы с секретами werf.

Идея заключается в том, что конфиденциальные данные должны храниться в репозитории вместе с приложением, и должны оставаться независимыми от какого-либо конкретного сервера.

{{ Рассказать про секретные переменные }}

#### Деплой в Gitlab CI

Опишем деплой приложения в Kubernetes. Деплой будет осуществляться на два стенда: staging и production.

Выкат на два стенда отличается только параметрами, поэтому воспользуемся шаблонами. Опишем базовый деплой, который потом будем кастомизировать под стенды: 

```
.base_deploy: &base_deploy
  script:
    - werf deploy --stages-storage :local
  dependencies:
    - Build
  tags:
    - article-werf
```

Выкат, например, на Staging, будет выглядеть так: 
 
 ```
 Deploy to Stage:
   extends: .base_deploy
   stage: deploy
   environment:
     name: stage
   except:
     - schedules
   only:
     - merge_requests
   when: manual
```

Нет необходимости пробрасывать переменные окружения, создаваемые GitLab CI — этим занимается Werf. Достаточно только указать название стенда

```yaml
environment:
     name: stage
```

_Обратите внимание: домены каждого из стендов указываются в helm-шаблонах._

_Остальные настройки подробно описывать не будем, разобраться в них можно с [помощью документации Gitlab](https://docs.gitlab.com/ce/ci/yaml/)_

После описания стадий выката при создании Merge Request и будет доступна кнопка Deploy to Stage.

![alt_text](images/-6.png "image_tooltip")

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**

![alt_text](images/-7.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**

![alt_text](images/-8.png "image_tooltip")

Из этого меню - можно так же быстро открыть приложение в браузере.

{{И тут в итоге должна быть картинка как аппка задеплоилась и объяснение картинки}}

# Подключаем зависимости

{{Что мы используем в качестве менеджера зависимостей в этой экосистеме.}}

{{Где оно хранит свои зависимости}}

{{Как происходит разворачивание зависимостей (какая-то команда в стиле npm install // composer install // …)}}

{{Нам не надо вызывать пересборку зависимостей на каждом коммите, но когда меняется вот этот конкретный файл — надо делать пересборку.}}

{{А ещё у этой штуки есть кэш, чтобы каждый раз менеджер зависимостей не тягал вообще всё каждый раз, он лежит в такой-то папке.}}

{{Для того, чтобы оптимизировать первое — юзаем стадии.}}

{{Рассказываем более глубоко про стадии и выбираем правильную.}}

{{А для того, чтобы с кэшем локальным работать есть артефакты, монтировать вот так-то.}}

# Генерируем и раздаем ассеты

{{Что мы берём в качестве генерации ассетов.}} 

{{Есть три вопроса связанных с генерацией ассетов:}}

{{*   Как описывается сценарий сборки ассетов}}

{{*   Как может конфигурироваться этот сценарий (какие конфиги зависят от окружения, какие — нет, как отличить одно от другого)}}

{{*   Куда складываются ассеты, в какую папку. И как потом ассеты раздаются.}}

{{    *   Какие изменения в сборку}}

{{    *   Какие изменения в делпой}}

{{    *   Какие изменения в ингрессе}}

## Какой сценарий сборки ассетов

{{Как происходит сборка ассетов, описан ли где-то этот процесс в виде кода? Или он прошит во фреймворке и нужно просто следовать каким-то правилам, и если это так — то где прочитать эти правила?}}

## Как конфигурируем сценарий сборки?

{{Про то, что есть два типа опций при сборке:}}

{{*   Те, что будут зависеть от окружения. И привести примеры таких вещей для вашего случая (урлики какие-нибудь в конфигах). }}

{{И объясняем, как мы подходим к конфигурированию таких вещей концептуально.}}

{{*   Те, что не зависят от окружения. И привести пример для вашего случая (например, это бывает волшебный хэшик для обхода кэша браузера) }}

{{И объясняем, как мы подходим к конфигурированию таких вещей концептуально.}}

{{*   Что-то ещё???}}

## Какие изменения необходимо внести

{{Концептуальное описание того, что у нас теперь два разных пода, два разных контейнера и как мы будем распределять между ними запросы и вот это всё.}}

### Изменения в сборке

{{Какие вносим изменения в стадию сборки. Подробности о том, как конкретно пробрасываем конфиги в стадию сборки.}}

### Изменения в деплое

{{Какие вносим изменения в стадию деплоя.}}

### Изменения в роутинге

{{Про ингрессы коротко.}}

# Работа с файлами и электронной почтой

{{Файлы – КАЗАЛОСЬ БЫ:}}

{{У вас в кубе есть сетевая файловая система (EFS, NFS, …), которая позволяет подключить общую директорию ко многим подам одновременно. Тогда можно работать по-старинке.}}

{{НО ЭТО НЕ ПРАВИЛЬНЫЙ ВАРИАНТ}}

{{Правильный вариант – S3.}}

{{Почта – один вариант, используем внешнее API.}}

# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем {{Python}} приложении корректно обратиться к этому приложению.

## Завести Redis в Kubernetes

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:

1. прописать изменения в yaml файлы; 
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл `.helm/requirements.yaml` следующие изменения:

```
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```

Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в `.gitlab-ci`

```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```

Опишем параметры для redis в файле `.helm/values.yaml`

```
redis:
  enabled: true
```

При использовании сабчарта по умолчанию создается master-slave кластер redis. 

Если посмотреть на рендер (`werf helm render`) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:

```
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```

## Подключение {{Python}} приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

{{Рассмотрим настройки подключения к redis из нашего приложения.}}

{{Тут дальше описываем как к какому порту подрубиться, чтобы подключиться. Возможно даже — куда мы и что пишем в приложении, чтобы это произошло.}}

# Подключаем базу данных

{{Для текущего примера в приложении должны быть установлены необходимые зависимости. В качестве примера - мы возьмем приложение для работы которого необходима база данных.}}


## Как подключить БД

{{То же, что в редисе, но просто другая бд, ну вы же не кретины, раз два три}}

## Выполнение миграций

{{Напоминаем, как мы запускаем миграции в этом фреймворке}}

{{Объясняем, куда мы это вписываем вот во всех этих сборках-деплоях и почему именно туда}}

## Накатка фикстур при первом выкате

{{Объясняем, куда это вписывать во всех тих сборках-деплоях.}}


# Юнит-тесты и Линтеры

{{Говорим, что за юнит-тесты/линтеры в этом фреймворке есть?}}

{{Объясняем, куда мы прописываем запуск этих тестов-линтеров и почему именно туда.}}

# Несколько приложений в одной репе

{{1. Добавляем кронджоб}}
{{2. Добавляем воркер/консюмер}}
{{3. Добавляем вторую приложуху на другом языке (например, это может быть webscoket’ы на nodejs; показать организацию helm, организацию werf.yaml, и ссылку на другую статью)}}

# Динамические окружения

{{TODO:}} 
