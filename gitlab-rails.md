---
author_team: "bravo"
author_name: "alexey.chazov"
ci: "gitlab"
language: "ruby"
framework: "rails"
is_compiled: 0
package_managers_possible:
 - bundler
package_managers_chosen: "bundler"
unit_tests_possible:
 - Rspec
unit_tests_chosen: "Rspec"
assets_generator_possible:
 - assets:precompile
assets_generator_chosen: "assets:precompile"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут ruby on rails программисту собрать приложение и запустить его в kubernetes кластете.

Предполагается что читатель имеет базовые знания в разработке на ruby on rails а также немного знаком с Gitlab CI и примитивами kubernetes. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - так же приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в кубах - если они соответствуют 12 факторам хероку. Благодаря этому - у нас в кубах работают stateless приложения который не зависят от среды в которой выполняются в текущий момент так как кластер куба может менять количество нод на которых в данный момент работает приложение или нам будет необходимо заказать ноды другой конфигурации или в другом регионе.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Тут наверно нужно что то сказать что у читателя должен быть уже куб

Запуск всех этапов сборки и деплоя приложения производится на gitlab runner. Как правило для этого используется виртуальная машина. 

Для пользователя под которым будет производиться запуск ранера - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:


```
. $(multiwerf use 1.1 stable --as-file)
```


Перед деплоем нашего приложения необходимо убедиться что у нас подготовлены инфраструктурные компоненты:



*   К gitlab подключен shell runner с тегом werf. [Инструкция](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке gitlab runner.
*   Ранеры включены и активны для репозитория с нашим приложением
*   Для пользователя под которым запускается сборка и деплой установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry


# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf.

В этом образе будет работать один основной процесс который запустит веб сервер для ruby. При запуске приложения команда запуска будет указана в файле с описанием деплоймента а управлять маршрутизацией запросов будет управлять ингресс в kubernetes кластере.


## Локальная сборка 

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки. 

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.

Создадим файл werf.yaml со следующим содержанием и дадим пояснения по его содержанию.


```
     1  project: example-1
     2  configVersion: 1
```


В первой строке мы указываем имя проекта. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

Во второй строке - версия конфиг файла werf.yaml. Эта статья написана для версии синтаксиса 1, список доступных версий будет всегда доступен по ссылке.


```
     4  {{ $_ := set . "RUBY_VERSION" "2.7.1" }}
```


С помощью переменной, указанной в строке  4 мы параметризовали дальнейшие инструкции по сборке. Таким образом можно указывать любые параметры для своей инструкции по сборке. По мере развития приложения данные параметры могут меняться, например повышаться версия ruby


```
     6  image: rails
     7  from: ruby:{{ .RUBY_VERSION }}
```


В строке 7 дано название для нашего собранного образа. Данное имя мы впоследствии будем указывать при запуске контейнера. Строка 7 определяет что будет взято за основу, мы берем официальный публичный образ с нужной нам версией ruby.


```
     8  docker:
     9    WORKDIR: /app
```


В строке 8 мы определяем переменные docker. В данном разделе мы можем указать например пользователя от которого производится запуск в docker контейнере.


```
    10  git:
    11  - add: /
    12    to: /app
    13    stageDependencies:
    14      install:
    15      - Gemfile
    16      - Gemfile.lock
    17      setup:
    18      - '*'
```


Далее в строках 10 - 18 мы описываем, как  исходный код нашего приложения попадёт в собираемый контейнер. Werf не просто копирует файлы, а тесно интегрирована с git:  добавляться и тратить время на сборку будут только те изменения, которые закомичены в git. Директива <code>[excludePaths](https://ru.werf.io/documentation/configuration/stapel_image/git_directive.html#%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%84%D0%B8%D0%BB%D1%8C%D1%82%D1%80%D0%BE%D0%B2)</code> указывает какие файлы будут проигнорированы при импорте — мы воспользуемся ей и её родственником includePaths позже, когда будем гибче настраивать сборку нескольких контейнеров. <code>[stageDependencies](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html)</code> позволяет указать при изменении каких файлов необходимо произвести пересборку стадии. Мы подробнее рассмотрим этот термин в следующих главах, когда будем глубже изучать вопросы зависимостей и ассетов, пока что не будем делать на этом акцент.


```
    19  ansible:
    20    beforeInstall:
    21    - name: install bundler
    22      shell: gem install bundler
    23    install:
    24    - name: bundle install
    25      shell: bundle config set without 'development test' && bundle install
    26      args:
    27        chdir: /app
```


В строках 19 - 27 описан процесс сборки зависимостей приложения.

Тут слова про то, что тут ансибл-инструкции.

Объяснение, что мы тут видим какие-то beforeInstall, install и прочие — это называется “стадии”, это важный механизм верфи, который позволяет настраивать стремительное ускорение сборки, но мы сейчас не будем на этом акценироваться и вернёмся к этому вопросу когда заговорим о зависимостях.

В данном примере мы используем ансибл-инструкцию shell, которая тупо выполняет шелл-команду. Но уоопсчето доступно большинство ансибл инструкций (вот тут ссылка на их список) и в более сложных случаях можно делать вещи типа

Вот тут пример с установкой например апт, или чего-то ещё.

В нашем хеллоу ворлд приложении пока что не нужен апт, потомушта мы взяли готовый образ с рельсами, но так бывает не всегда и в реальной практике часто приходится собирать свои образы ((??? Это так или я напиздел???))

Этого достаточно чтобы производить дальнейшие действия с приложением. Запустить локальную сборку можно с помощью команды.


```
$ werf build --stages-storage :local
```


В результате будет собран набор образов, которые можно увидеть с помощью команды docker ps. Впрочем, напрямую через docker мы, конечно, с ним работать не будем - так как всеми наборами образов управляет werf загружая по сети только те части, которые необходимо использовать в дальнейшем.

Для отладки процесса сборки можно воспользоваться ключами … которые позволят перейти в командную строку на той стадии - где возникла ошибка. В таком случае вы можете провалиться в контейнер и скорректировать команды чтобы добиться корректной сборки приложения.


## Сборка в Gitlab CI

Далее нам нужно автоматизировать сборку нашего приложения. Для этого этапа - мы договоримся что наше приложение лежит в gitlab репозитории, к данному репозиторию подключен runner и на нем установлен werf.

Создадим файл gitlab-ci.yml, в котором на текущий момент опишем только стадию сборки и пуша в gitlab registry


```
     4  variables:
     5      WERF_STAGES_STORAGE: ":local"
```


Переменная <code>[WERF_STAGES_STORAGE](https://ru.werf.io/documentation/reference/stages_and_images.html#%D1%85%D1%80%D0%B0%D0%BD%D0%B8%D0%BB%D0%B8%D1%89%D0%B5-%D1%81%D1%82%D0%B0%D0%B4%D0%B8%D0%B9)</code> указывает где werf сохраняет свой кэш (стадии сборки) У werf есть опция распределенной сборки, про которую вы можете прочитать в нашей статье, в текущем примере мы сделаем по-простому и сделаем сборку на одном узле в один момент времени.


```
     7  before_script:
     8    - type multiwerf && source <(multiwerf use 1.1 stable)
     9    - type werf && source <(werf ci-env gitlab --verbose)
```


Инициализируем werf перед запуском основной команды. Это необходимо делать перед каждым использованием werf поэтому мы вынесли в секцию `before_script`

Такой сложный путь с использованием multiwerf нужен для того, чтобы вам не надо было думать про обновление верфи и установке новых версий — вы просто указываете, что используете, например, use 1.1 stable и пребываете в уверенности, что у вас актуальная версия с закрытыми issues.


```
    11  Build:
    12    stage: build
    13    script:
    14      - werf build-and-publish
    15    tags:
    16      - werf
    17    except:
    18      - schedules
```


Основная команда на текущий момент - это werf build-and-publish, которая запускает сборку и публикацию в registry на gitlab runner с тегом werf для любой ветки. Путь до registry и другие параметры беруться верфью автоматически их переменных окружения gitlab ci.

Если вы всё правильно сделали и корректно настроен registry и gitlab ci — вы увидите собранный образ в registry. При использовании registry от gitlab — собранный образ можно увидеть через веб-интерфейс гитлаба.

&lt;картинка-пример>


## Деплой

werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как — файлы секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

При деплое приложения конструкция {{ .Chart.Name }} будет заменена значением project из werf.yaml

Верфь использует хельм чарты. Они лежат в .helm/templates в виде yaml файлов со специальным синтаксисом о котором по ссылке.

Как устроен деплой:


    Werf (по аналогии с helm) берет эти yaml шаблоны


    Генерирует из них  огромную простыню с финальными ямлами, куда подставлены все значения. В этой простыне ямла — аннотации для кубернетеса


    Эта простыня закидывается в кубернетес кластер, который парсит инструкции в ямле и вносит изменения в кластер


    Верфь **<span style="text-decoration:underline;">смотрит за тем, как кубернетес вносит изменения</span>** и дожидается, чтобы реально всё было применено.


## Подготовка к деплою

Для начала напишем шаблоны в хельм-синтаксисе.

Создадим директорию .helm/templates и опишем в ней все необходимые нам yaml файлы. 

В этой папке находятся YAML-файлы *.yaml, каждый из которых описывает один или несколько ресурсов Kubernetes, разделенных тремя дефисами ---, например:

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm
├── templates
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файд с пеоеменными)
```


 

Каждый YAML-файл предварительно обрабатывается как Go-шаблон.

Использование Go-шаблонов дает следующие возможности:



*   генерирование разных спецификаций объекта Kubernetes в зависимости от какого-либо условия;
*   передача данных в шаблон, в зависимости от окружения;
*   выделение общих частей шаблона в блоки и их переиспользование в нескольких местах;


### Общее

Для дальнейшей работы с приложением, необходимо понимать как работать с helm шаблонами. 

Go-шаблоны имеют большой порог вхождения, однако никаких ограничений по возможностям и проблем с DRY у технологии нет. Основные принципы, синтаксис, функции и операторы разбираются в нашей [предыдущей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

Наверное надо бросить ссылку на то, как вкурить в хельм (он чудовищно сложен для разработчика) и чтобы если чувак не знает примитивов куберенетса - шёл куда-то учить в правильном направлении.

А ещё надо указать на САМЫЕ частые проблемы при работе с этими ямликами:



*   Про отступы и indent за которыми надо следить. 90% ошибок в начале работы с кубом
*   Про лейблы коротко, которые генерируют вторые 90% ошибок в начале работы с кубом. Что их надо палить внимательно и если чёто не получается - вот тут читайте и вот туда копайте.
*   ???


### Приложение

В первом файле `.helm/templates/deployment.yaml` описан деплоймент и сервис для нашего контейнера.


```
    17        containers:
    18        - name: rails
    19          command: ["bundle", "exec", "rails", "server", "-b", "0.0.0.0"]
    20  {{ tuple "rails" . | include "werf_container_image" | indent 8 }}
```


В строке 19 указываем команду запуска для контейнера — это стандартный синтаксис хельма

В строке 20 задействована функция которая подставит необходимый нам образ с приложением.

Данная функция генерирует ключи image и imagePullPolicy со значениями, необходимыми для соответствующего контейнера пода.

Особенность функции в том, что значение imagePullPolicy формируется исходя из значения .Values.global.werf.is_branch. Если не используется тег, то функция возвращает imagePullPolicy: Always, иначе (если используется тег) — ключ imagePullPolicy не возвращается. В результате образ будет всегда скачиваться если он был собран для git-ветки, т.к. у Docker-образа с тем же именем мог измениться ID.

Функция может возвращать несколько строк, поэтому она должна использоваться совместно с конструкцией indent. Подробнее - можно посмотреть в [документации](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_image).


```
    21          env:
    22  {{ tuple "rails" . | include "werf_container_env" | indent 8 }}
    23          - name: RAILS_MASTER_KEY
    24            value: {{ .Values.rails.master_key}}
    25          - name: RAILS_ENV
    26            value: production
    27          - name: RAILS_LOG_TO_STDOUT
    28            value: "true"
```


Для того чтобы логи приложения отправлялись в stdout нам необходимо будет добавить переменную окружения `RAILS_LOG_TO_STDOUT="true" `согласно [изменениям](https://github.com/rails/rails/pull/23734) в rails framework.

В строках 25 и 27 мы указываем переменные окружения которые определяют environment для запускаемого приложения и перенаправляют логи в stdout. 

Подобным образом передаются и другие значения из файлов values.yaml и secret-values.yaml.

Конструкция с  [werf_container_env](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_env) позволяет упростить процесс релиза, в случае если образ остается неизменным. Возвращает блок с переменной окружения DOCKER_IMAGE_ID контейнера пода. Значение переменной будет установлено только если .Values.global.werf.is_branch=true, т.к. в этом случае Docker-образ для соответствующего имени и тега может быть обновлен, а имя и тег останутся неизменными. Значение переменной DOCKER_IMAGE_ID содержит новый ID Docker-образа, что вынуждает Kubernetes обновить объект.

Описание порта который будет доступен через сервис.


```
    29          ports:
    30          - containerPort: 3000
    31            name: http
    32            protocol: TCP
```


Не забываем создать валидный ключ для доступа из kubernetes к registry gitlab.


```
    15        imagePullSecrets:
    16        - name: registrysecret
```


В файле `.helm/templates/service.yaml` описан сервис через который будет доступно наше приложение.

Порт и сервис нам очень нужны и важны, так как именно на них мы будем отправлять запросы, когда будем конфигурировать ingrss.


### Роутинг

Создадим ingress `.helm/templates/ingress.yaml `для нашего `kubernetes.io/ingress.class.` 

_Тут есть коллизия терминов “ингресс” - в смысле приложение-балансировщик, которое работает в кластере и принимает входящие извне запросы и “Ингресс” в смысле аннотация, которую мы скармливаем кубернетесу, чтобы он настроил ингресс-приложение. Смиритесь, разбирайтесь по контексту._


```
    53    rules:
    54    - host: {{ .Values.global.ci_url }}
    55      http:
    56        paths:
    57        - path: /
    58          backend:
    59            serviceName: {{ .Chart.Name }}
    60            servicePort: 3000
```


Обратите внимание на параметр {{ .Values.global.ci_url }}. Данный параметр передается из файла .gitlab-ci.yml


```
.base_deploy:
  script:
    - werf deploy
      --set "global.ci_url=example.com"
```


Подобным образом, можно передавать и другие необходимые переменные. 


### Проверка шаблонов

Проверить какие файлы получаются в результате работы werf с шаблонами можно выполнив команду рендер.


```
$ werf helm render
```



### Секретные переменные

Для хранения в репозитории паролей, файлов сертификатов и т.п., рекомендуется использовать подсистему работы с секретами werf.

Идея заключается в том, что конфиденциальные данные должны храниться в репозитории вместе с приложением, и должны оставаться независимыми от какого-либо конкретного сервера.

Для работы нашего приложения потребуется `RAILS_MASTER_KEY` с помощью которого происходит шифрование секретов и оно необходимо в production окружении.

Данную переменную окружения мы уже указали в деплойменете нашего приложения в строке 24. Все секретные данные - нужно описывать в зашифрованом файле `.helm/secret-values.yaml` 

Подстановка значений из этого файла происходит при рендере шаблона, который также запускается при деплое.

Для создания зашифрованных данных нам необходимо сгенерировать ключ.


```
$ werf helm secret generate-secret-key
504a1a2b17042311681b1551aa0b8931
```


После генерации ключа - необходимо указать его в переменных окружения или в файле в корне нашего приложения


```
$ export WERF_SECRET_KEY=504a1a2b17042311681b1551aa0b8931 
OR
$ echo 504a1a2b17042311681b1551aa0b8931 > .werf_secret_key
```


Теперь можем редактировать наши секретные переменные:


```
$ werf helm secret values edit .helm/secret-values.yaml
```


Откроется текстовый редактор который указан в `$EDITOR` или vim по умолчанию.

После шифрования наш файл будет выглядеть следующем образом:


```
rails:
  master_key: 100083f330adfb9e13fff74c9ab71b93ed77704aca3a0c607679336e099d48977d6565b314c06b8ad7aefc9d8d90629e92d851b573a89915ff036239de129d722ef5
```


Для декодирования секретных переменных необходимо добавить переменную `WERF_SECRET_KEY` в Variables  в для репозитория Settings - CI / CD. Гитлаб пробросит эту переменную в раннер и когда мы в нашем gitlab ci вызываем верфь деплой — werf увидит это значение и производит расшифровку секретных переменных.



<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/-0.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/-0.png "image_tooltip")



### Gitlab CI: Деплой

Для того чтобы запустить приложение в нашем kubernetes кластере - нам осталось описать стадию деплоя в Gitlab CI

Мы будем делать два стенда.

Магия кубернетеса поможет нам создать новый стенд **_просто прописав это в gitlab-ci_**. Вся остальная магия произойдёт сама, если всё правильно сконфигурировано и на кластер куберенетса завернут днс правильно.


```
.base_deploy:
  stage: deploy
  script:
    - werf deploy
      --set "global.ci_url=$(echo ${CI_ENVIRONMENT_URL} | cut -d / -f 3)"
  dependencies:
    - Build
  tags:
    - werf
  except:
    - schedules

Deploy to stage:
  extends: .base_deploy
  environment:
    name: stage
    url: http://example-1-stage.example.com
  only:
    - master

Deploy to production:
  extends: .base_deploy
  environment:
    name: production
    url: http://example-1.example.com
  only:
    - master
  when: manual
```


В данном файле мы описали базовые правила для деплоя (base_deploy) и расширили их на выкат в 2 окружения (у нас это будут разные namespaces в kubernetes). Для данного приложения мы хотим производить выкат из master ветки автоматически в stage окружение и вручную в production. Environment.url передается в base_deploy и преобразуется в hostname без указания схемы. Данный подход позволяет воспользоваться быстрым открытием нашего приложения в браузере из интерфейса gitlab. 

После пуша всех изменений в gitlab - у нас автоматически запускается сборка и деплой приложения в наш kubernetes кластер. 

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**



<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/-1.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/-1.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**



<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/-2.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/-2.png "image_tooltip")


Из этого меню - можно так же быстро открыть приложение в браузере.


# Подключаем зависимости

Werf подразумевает, что лучшей практикой будет разделить сборочный процесс на этапы, каждый с четкими функциями и своим назначением. Каждый такой этап соответствует промежуточному образу, подобно слоям в Docker. В werf такой этап называется стадией, и конечный образ в итоге состоит из набора собранных стадий. Все стадии хранятся в хранилище стадий, которое можно рассматривать как кэш сборки приложения, хотя по сути это скорее часть контекста сборки.

Стадии — это этапы сборочного процесса, кирпичи, из которых в итоге собирается конечный образ. Стадия собирается из группы сборочных инструкций, указанных в конфигурации. Причем группировка этих инструкций не случайна, имеет определенную логику и учитывает условия и правила сборки. С каждой стадией связан конкретный Docker-образ. Подробнее о том, какие стадии для чего предполагаются можно посмотреть в [документации](https://ru.werf.io/documentation/reference/stages_and_images.html).

Werf предлагает использовать для стадий следующую стратегию:



*   использовать стадию beforeInstall для инсталляции системных пакетов;
*   использовать стадию install для инсталляции системных зависимостей и зависимостей приложения;
*   использовать стадию beforeSetup для настройки системных параметров и установки приложения;
*   использовать стадию setup для настройки приложения.

Подробно про стадии описано в [документации](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html).


```
ansible:
  beforeInstall:
  - name: install bundler
    shell: gem update --system && gem install bundler:{{ .BUNDLER_VERSION }}
  install:
  - name: bundle install
    shell: bundle config set without 'development test' && bundle install
    args:
      chdir: /app
```


Зависимость пользовательской стадии от изменений в git-репозитории указывается с помощью параметра git.stageDependencies.


```
git:
- add: /
  to: /app
  stageDependencies:
    install:
    - Gemfile
    - Gemfile.lock
```


При изменении файлов Gemfile или Gemfile.lock стадия install будет запущена заного а не взята из кэша (хранилища стадий)

Дополнительную информацию про stageDependencies можно прочитать в [документации](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html#%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C-%D0%BE%D1%82-%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B2-git-%D1%80%D0%B5%D0%BF%D0%BE%D0%B7%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%B8)


# Генерируем и раздаем ассеты

В какой-то момент в процессе разработки вам понадобятся ассеты (т.е. Картинки, css, js).

Для того, чтобы мы могли использовать ассеты, нам нужно будет внести изменения в сборку и деплой.

Интуитивно понятно, что на стадии сборки нам надо будет вызвать скрипт, который генерирует файлы, т.е. Что-то надо будет дописать в werf.yaml. Однако, не только там — ведь какое-то приложение в production должно непосредственно отдавать статические файлы. Мы не будем отдавать файлики рельсами. Хочется, чтобы статику раздавал нгинкс. А значит надо будет внести какие-то изменения и в хельм чарты.


### Сборка

В данной главе мы воспользуемся приложением для работы которого нужна генерация ассетов. 

Генерация ассетов происходит в артефакте на стадии setup, так как данная стадия рекомендуется для настройки приложения


```
  setup:
  - name: build assets
    shell: RAILS_ENV=production SECRET_KEY_BASE=fake bundle exec rake assets:precompile
    args:
      chdir: /app
```


Тут есть один нюанс - при сборке приложения мы не рекомендуем использовать какие-либо изменяемые переменные. Потому что собранный бинарный образ должен быть независимым от конкретного окружения. А значит во время сборки у нас не может быть, например, базы данных, user-generated контента и подобных вещей. Но по непонятной причине - для генерации assets rails ходит в базу данных, хотя не понятно для каких целей и для этого - нужен SECRET_KEY_BASE​. При текущей сборке - мы использовали workaround, передав фейковое значение. По этому поводу есть issue созданное более 2х лет назад, но в версии rails 2.7 - до сих пор так. Если вы знаете, зачем авторы Rails так сделали - просьба написать в комментариях.

Окей, а в каком контейнере в конечном итоге должны оказаться собранные файлы? Есть минимум два варианта:



*   Делать один образ в котором: рельсы, сгенерированные ассеты, нгинкс. Запускать этот один и тот же образ двумя разными способами (с разным исполняемым файлом)
*   Делать два образа: рельсы отдельно, nginx + сгенерированные ассеты отдельно.

В первом варианте при каждом изменении будут перекатываться оба контейнера. Такое себе в большинстве случаев.

Пойдём вторым путём.

Дальше сложности, ибо для сборки нужны рельсы, нода, но в финальном образе не хочется иметь вот этого всего дерьма: нам в финальном образе нужна только статика и нгинкс.

И ВОТ ТУТ нам на помощь приходят артефакты. Мы ВОТ ТУТ объясняем что такое артефакты и поясняем, что мы сможем сгенерить в одном а пихнуть в другое и финальный образ будет вжух шустрый быстрый маленький охуеннный.

И рассказываем как конкретно будем собирать

В образе с нашим приложением мы не хотим чтобы у нас была установлена среда для сборки приложения и nginx а также для того чтобы уменьшить размеры образов - мы воспользуемся сборкой с помощью артефактов.

[Артефакт](https://ru.werf.io/documentation/configuration/stapel_artifact.html) — это специальный образ, используемый в других артефактах или отдельных образах, описанных в конфигурации. Артефакт предназначен преимущественно для отделения ресурсов инструментов сборки от процесса сборки образа приложения. Примерами таких ресурсов могут быть — программное обеспечение или данные, которые необходимы для сборки, но не нужны для запуска приложения, и т.п.

С помощью такого подхода мы сможем собрать и подготовить все файлы и зависимости в одном образе и импортировать нужные нам файлы по двум разным docker контейнерам, где в одном - будет среда для выполнения приложения ruby on rails а во втором - только ngin со статическими файлами.

Подготовленные ассеты мы будет отдавать через отдельный nginx контейнер в поде чтобы не загружать основное приложение лишними подключениями. Для этого так-же произведем импорт подготовленных файлов в отдельный образ.


```
---
image: assets
from: nginx:alpine
ansible:
  beforeInstall:
  - name: Add nginx config
    copy:
      content: |
{{ .Files.Get ".werf/nginx.conf" | indent 8 }}
      dest: /etc/nginx/nginx.conf
import:
- artifact: build
  add: /app/public
  to: /www
  after: setup
```


При работе мы планируем, что все запросы будут проксироваться через nginx, поэтому заменяем файл /etc/nginx/nginx.conf на необходимый нам, который находится также в репозитории с приложением. Такой подход позволит нам управлять лимитом подключений который может принять приложение.


### Изменения в деплой

При таком подходе изменим деплой нашего приложения добавив еще один контейнер в наш деплоймент с приложением.  Укажем livenessProbe и readinessProbe, которые будут проверять корректную работу контейнера в поде. preStop команда необходима для корректного завершение процесса nginx. В таком случае при новом выкате новой версии приложения будет корректное завершение всех активных сессий.


```
      - name: assets
{{ tuple "assets" . | include "werf_container_image" | indent 8 }}
        lifecycle:
          preStop:
            exec:
              command: ["/usr/sbin/nginx", "-s", "quit"]
        livenessProbe:
          httpGet:
            path: /healthz
            port: 80
            scheme: HTTP
        readinessProbe:
          httpGet:
            path: /healthz
            port: 80
            scheme: HTTP
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
```



# Работа с файлами и электронной почтой

Если нам необходимо сохранять какие то пользовательские данные - нам нужно  персистентное хранилище. Лучше всего для stateless приложений в таком случае использовать S3 совместимое хранилище (например minio или aws s3)

Данная настройка производится полностью в рамках приложения а нам остается только передать необходимые переменные окружения при запуске приложения.

Данный пример приложения будет примерах.

Если мы будем сохранять файлы какой - либо директории у приложения запущенного в kubernetes - то после перезапуска контейнера все изменения пропадут.

Работа с электронной почтой производится с помощью внешнего api например mailgun


# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем Rails приложении корректно обратиться к этому приложению.


## Завести редис в кубернетес

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:



1. прописать изменения в yaml файлы; 
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл .helm/requirements.yaml следующие изменения:


```
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```


Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в .gitlab-ci


```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```


Опишем параметры для redis в файле `.helm/values.yaml`


```
redis:
  enabled: true
```


При использовании сабчарта по умолчанию создается master-slave кластер redis. 

Если посмотреть на рендер (werf helm render) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:


```
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```



## Подключение Rails приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

Рассмотрим настройки подключение к redis из нашего приложения на примере стандартного cable (config/cable.yml)


```
production:
  adapter: redis
  url: <%= ENV.fetch("REDIS_URL") { "redis://localhost:6379/1" } %>
  channel_prefix: example_2_production
```


В данном файле мы видим что адрес подключения берется из переменной окружения `REDIS_URL` и если такая переменная не задана - подставляется значение по умолчанию `redis://localhost:6379/1`

Для подключения нашего приложения к redis нам необходимо добавить в список зависимостей `gem 'redis', '~> 4.0'` и указать переменную окружения `REDIS_URL` при деплое нашего приложения в файле с описанием деплоймента.


```
- name: REDIS_URL
  value: "redis://{{ .Chart.Name }}-{{ .Values.global.env }}-redis-master:6379/1"
```


В итоге, при деплое нашего приложения преобразуется например в строку

`redis://example-2-stage-redis-master:6379/1 `для stage окружения 


# Подключаем базу данных

Для текущего примера в приложении должны быть установлены необходимые зависимости. В качестве примера - мы возьмем приложение для работы которого необходима база данных.


## Завести postgresql в кубернетес

Подключим postgresql helm сабчартом, для этого внесем изменения в файл `.helm/requirements.yam.`


```
dependencies:
- name: postgresql
  version: 8.0.0
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: postgresql.enabled
```


Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в .gitlab-ci


```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```


Опишем параметры для postgresql в файле `.helm/values.yaml`


```
postgresql:
  enabled: true
  postgresqlDatabase: hello_world
  postgresqlUsername: hello_world_user
  postgresqlHost: postgres
  imageTag: "12"
  persistence:
    enabled: true
```


Пароль от базы данных добавим в `secret-values.yaml`


## Подключение Rails приложения к базе postgresql

Настройки подключения нашего приложения к базе данных мы будем передавать через переменные окружения. Такой подход позволит нам использовать один и тот же образ в разных окружениях, что должно исключить запуск непроверенного кода в production окружении.

Внесем изменения в файл настроек подключения к базе данных


```
$ cat config/database.yml
default: &default
  adapter: postgresql
  encoding: unicode
  pool: <%= ENV.fetch("RAILS_MAX_THREADS") { 5 } %>
  url: <%= ENV['DATABASE_URL'] %>
  database: <%= ENV['DATABASE_NAME'] %>

development:
  <<: *default
test:
  <<: *default
production:
  <<: *default
```


Параметры подключения приложения к базе данным мы опишем в файле `.helm/templates/_envs.tpl`


```
{{- define "database_envs" }}
- name: DATABASE_URL
  value: "postgres://{{ .Values.postgresql.postgresqlUsername }}:{{ .Values.postgresql.postgresqlPassword }}@{{ .Chart.Name }}-{{ .Values.global.env }}-postgresql:5432"
- name: DATABASE_NAME
  value: {{ .Values.postgresql.postgresqlDatabase }}
{{- end }}
```


Такой подход позволит нам переиспользовать данное определение переменных окружения для нескольких контейнеров. Имя для сервиса postgresql генерируется из названия нашего приложения, имени окружения и добавлением postgresql

Остальные значения подставляются из файлов values.yaml и secret-values.yaml


## Выполнение миграций

Запуск миграций производится созданием приметива Job в kubernetes. Это единоразовый запуск пода с необходимыми нам контейнерами.

Добавим запуск миграций после каждого деплоя приложения.


```
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $.Values.global.werf.name }}-migrate-db
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-weight": "2"
spec:
  backoffLimit: 0
  template:
    metadata:
      name: {{ $.Values.global.werf.name }}-migrate-db
    spec:
      initContainers:
      - name: wait-postgres
        image: postgres:12
        command:
          - "sh"
          - "-c"
          - "until pg_isready -h {{ .Chart.Name }}-{{ .Values.global.env }}-postgresql -U {{ .Values.postgresql.postgresqlUsername }}; do sleep 2; done;"
      containers:
      - name: rails
{{ tuple "rails" . | include "werf_container_image" | indent 8 }}
        command: ["bundle", "exec", "rake", "db:migrate"]
        env:
{{- include "apps_envs" . | indent 10 }}
{{- include "database_envs" . | indent 10 }}
{{ tuple "rails" . | include "werf_container_env" | indent 10 }}
      restartPolicy: Never
```


Аннотации `"helm.sh/hook": post-install,post-upgrade` указывают условия запуска job а `"helm.sh/hook-weight": "2"` указывают на порядок выполнения (от меньшего к большему)

При запуске миграций мы используем тот же самый образ что и в деплойменте. Различие только в запускаемых командах.


# Юнит-тесты и Линтеры

Запуск тестов и линтеров - это отдельные стадии в piplinе для выполнения которых могут быть нужны определенные условия.

Если мы хотим воспользоваться пакетом rubocop-rails нам нужно добавть эту зависимость в наше приложение, собрать образ приложения и запустить выполнение задания отдельной стадией на нашем gitlab runner командной [werf run](https://ru.werf.io/documentation/cli/main/run.html).


```
Rubocop check:
  script:
    - werf run rails -- rubocop --require rubocop-rails
```


При таком запуске наш kubernetes кластер не задействован.

Если нам нужно проверить приложение линтером, но данные зависимости не нужны в итоговом образе - нам необходимо собрать отдельный образ. Данный пример будет в репозитории с примерами а тут мы его не будем описывать.


# Несколько приложений в одной репе

Если в одном репозитории находятся несколько приложений например для backend и frontend необходимо использовать сборку приложения с несколькими образами.

Мы рассказывали [https://www.youtube.com/watch?v=g9cgppj0gKQ](https://www.youtube.com/watch?v=g9cgppj0gKQ) о том, почему и в каких ситуациях это — хороший путь для микросервисов.

Покажем это на примере приложения на rails запросы на которое отправляются по /api и приложением на react которое отображает web часть.


## Сборка приложений

Сборка приложения с несколькими образами описана в [статье](https://ru.werf.io/documentation/guides/advanced_build/multi_images.html). На ее основе покажем наш пример для нашего приложения.

Структура каталогов будет организована следующим образом


```
├── .helm
│   ├── templates
│   └── values.yaml
├── backend
├── frontend
└── werf.yaml
```


Сборка приложения для api практически не отличается от сборки описанной в Hello World, за исключением того что импорт из git будет из директории backend

Сборка для frontend приложения описана в файле werf.yaml как отдельный образ


```
image: frontend
from: node
git:
- add: /frontend
  to: /app
ansible:
  beforeInstall:
  - name: install dependencies
    apt:
      name:
      - yarn
  install:
  - name: install node dependencies
    shell: yarn install
    args:
      chdir: /app
  setup:
  - name: build js app
    shell: yarn build
    args:
      chdir: /app
```


Сборка для backend приложения описана в файле werf.yaml так же как отдельный образ.


```
image: backend
from: rails
git:
- add: /backend
  to: /app
ansible:
  install:
  - name: bundle install
    shell: |
      bundle install --without development test
    args:
      chdir: /app
```


Для запуска подготовленных приложений отдельными деплойментами, необходимо создать 2 файла, один для frontend другой для backend. Маршрутизация запросов будет осуществляться через ingress.


```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Chart.Name }}-frontend
  labels:
    app: {{ .Chart.Name }}-frontend
spec:
  selector:
    matchLabels:
      app: {{ .Chart.Name }}-frontend
  template:
    metadata:
      labels:
        app: {{ .Chart.Name }}-frontend
    spec:
      containers:
      - name: backend
{{ include "volume_mounts_envs" . | indent 8 }}
{{ tuple "frontend" . | include "werf_container_image" | indent 8 }}
...
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Chart.Name }}-backend
  labels:
    app: {{ .Chart.Name }}-backend
spec:
  selector:
    matchLabels:
      app: {{ .Chart.Name }}-backend
  template:
    metadata:
      labels:
        app: {{ .Chart.Name }}-backend
    spec:
      containers:
      - name: backend
{{ tuple "backend" . | include "werf_container_image" | indent 8 }}
...
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: {{ .Chart.Name }}
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: frontend
          servicePort: 80
      - path: /api/
        backend:
          serviceName: backend
          servicePort: 3000
```


Таким образом мы смогли собрать и запустить несколько приложений написанных на разных языках которые находятся в одном репозитории.

Если в вашей команды фуллстэки и/или она не очень большая и хочется видеть и катать приложение целиком, может быть полезно разместить приложения на нескольких языках в одной репке.

К слову, мы рассказывали [https://www.youtube.com/watch?v=g9cgppj0gKQ](https://www.youtube.com/watch?v=g9cgppj0gKQ) о том, почему и в каких ситуациях это — хороший путь для микросервисов.

В реальной ситуации таким “вторым” приложением при разработки на Rails может стать полностью автономный фронтэнд на JS, который работает с Rails-приложением по API. Но мы для примера используем гиперболизированный пример и добавим PHP-приложение Hello world — технически разница не велика.

…..


# Динамические окружения

Если для командной работы большой группе разработчиков необходимо проверять и делиться своими разработками с другими членами команды - можно воспользоваться динамическими окружениями.

Плюсом использования такого подхода - является то что если у нас приложение уже подготовлено для запуска в kubernetes - то нам нужно только добавить несколько стадий в ci

Рассмотрим пример деплоя 


```
Deploy to future:
  extends: .base_deploy
  stage: deploy
  environment:
    name: ${CI_COMMIT_REF_SLUG}
    url: http://${CI_COMMIT_REF_SLUG}.k8s.example.com
  only:
  - future/*
  when: manual
```


При таком ci - мы можем выкатывать каждую ветку future/* в отдельный namespace с изолированной базой данных, накатом необходимых миграций и например проводить тесты для данного окружения.

В репозитории с примерами будет реализовано отдельное приложение которое показывает реализацию данного подхода.
